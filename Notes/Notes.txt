On "A Survey on Explainability of Graph Neural Networks"
- https://arxiv.org/pdf/2306.01958

Complex non-linear GNNs -> Need for interpretability

Specially, when it comes to drug discovery,
understanding the "decision landscape" could
lead to breakthroughs or atleast better heuristics.

-> Challenges with GNN explainability:
	- Graphs are combinatorial data structures, and as such it is difficult to find substructures that maximize a certain predictions (this by doing combinatorial exploration).

	- Need to consider node attributes, and edge connectivity together in the explanations.

	- These explanations need cover all other GNN architectures.

	- Local (node and edge level) and global (graph level) explanations may differ.

	- GNN explanability is non-trivial therefore.


Problem definition _Explainability_:
	- Consider a supervised task T, such that we want to learn the mapping from X to Y, with a model M. Given a set of pairs (x,y) belonging to (X,Y) with model M, we want to generate an explanation e such that e "explains" output = M(x).


These may be local if they explain a prediction for a single input (x,y) or global if they happen to explain the prediction over a specific dataset.

Explanation might be post-hoc (generated after training) or ante-hoc (model explains its own predictions, self-interpretable).

Explanations might also be divided into factual and counterfactual. In counterfactual explanations, a prediction is explained by providing a contrasting example that changes it.


Factual explanations rely on finding the input features that have the maximum influence over prediction. Imagine a set of node features, or a graph substructure.

Counterfactual ones rely on finding the smallest change on the graph that changes the model's prediction (Here thinking about classification problems, presumably. This type of approach for regression problems would be much harder I assume, and you would need to define an reasonable threshold for which you would consider two predictions to be different.)


####

Factual explanations can be:
	- Post-hoc:
		The explainable architecture isn't built into the model. So Post-hoc methods generate an explanation based on the model's input, output, and sometimes on some of its params. Post-hoc methods are further divided into: Decomposition-based methods, Gradient-based methods, Surrogate methods, Pertubation-based methods, Generation-based methods. Furthermore, if we discriminate based on the requirement to access the model's internal params, we have: white-box (require access to internal params or embeddings, i.e. Decomposition-based methods that need access to node weights of each layer; Gradient-based methods that need access to gradients) and black-box (No need for internal params of the model. Surrogate methods for example, generate a local dataset with the model's input and output.)

		- Decomposition-based methods: There are multiple Decomposition-based methods, however all of them consider the prediction of the model as a score that is decomposed and distributed backwards in a layer by layer manner, till it reaches the input layer.

		- Gradient-based methods: Importance in these types of methods is taken as the sensitivity. In this case, how sensitive is the prediction wrt the input. Better yet, the sensitivity is the gradient of the prediction wrt the input.

		- Surrogate methods: For a large range of inputs, the relationship input-output can become very complex. As such, for us to model this complex relationship, we would need more complex functions, for which the corresponding model will become uninterpretable. For a smaller range of input values though, this relationship input-output can be approximated by simpler and interpretable functions. Surrogate methods then, will fit a simple yet interpretable surrogate model within the locality of the prediction. These methods work in two-step manner to generate explanations. Given an instance G, they generate data from the prediction's neighborhood by utilizing multiple inputs D in the vicinity and recording the model's prediction Y. Then, a surrogate model is used to train on this data, and the explanation generated by the surrogate model is used as an explanation for the original prediction.

		- Pertubation-based methods: These methods find important subgraphs as explanations by perturbing the input. For an input G, the subgraph extraction module extracts a subgraph G_s, which is then used for a prediction Y_s using the respective model, and compared with a scoring function, against the actual prediction Y. The feedback from the extraction module can then be used to train the subgraph extraction module. Sometimes, some model's params can be also be provided as training input for the extraction module, such that these methods overall present explanations in the form of subgraph structures, with some also providing these as node features.

		- Generation-based methods: These approaches use either generative models or graph generators to derive instance-level or model-level explanations.
	
	- Self-interpretable: 
		In self-interpretable methods, the explanation is intrinsic to the model. This is done by implementing interpretability constraints. We have information constraints or structural (cardinal) ones to derive an information subgraph which is used both for the prediction and explanation.
		# These could be descrined more in depth.

Counterfactual explanations can be:
	Provide an explanation by indentifying the minimal alteration in the input graph, such that it changes the model's prediction.

	- Pertubation-based:
		Example: Alter the edges (delete or add them in the respective graph) -> Perturb the adjacency matrix. Can be done with a binary mask (0 or 1) P to acess which edges to pertub. Probably not the best for use in molecular graphs, if there aren't restrictions, as we need to obey valence restrictions and adding or deleting edges in this manner might lead to non-realizable molecules.
	
	- Neural framework-based: 
		As by the name, counterfactual graphs are generated by neural nets.

	- Search-based:
		Depend on search techinques to explore the counterfactual space. i.e. for a certain task a molecule is labeled as non-effective, and we would want to find a similar yet active molecule.


Another important type of explainer is that of causality-based explainers. Usually, most of the GNN classifiers learn the statistical correlation between input features and output, whilst not differentiating between causal and non-causal features. As such, prediction majority of the times may be heavily using shortcut features, which are confounders between the causal features and the prediction. These methods attempt to reduce this confounder effect.